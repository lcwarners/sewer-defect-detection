{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a412e4ec-aac4-4609-b371-0cee07647cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from moviepy.editor import VideoFileClip\timport cv2 as cv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, Dropout, Input, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, AveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ac0e946-9b96-4e97-b10d-c7d4c274e25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the three following cells are used to recreate the model for which the weights were calculated\n",
    "efficientnet_model = tf.keras.applications.efficientnet.EfficientNetB1(include_top=False,input_tensor=Input(shape=(224, 224, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cd68bbc-73e7-4867-80c3-47260a464b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_layers = efficientnet_model.output\n",
    "top_layers = Flatten(name=\"flatten_top\")(top_layers)\n",
    "top_layers = Dense(1024, activation=\"ReLU\",name=\"first_dense_top\")(top_layers)\n",
    "top_layers = Dropout(0.5, name=\"dropout_top\")(top_layers)\n",
    "top_layers = Dense(units=3, activation=\"softmax\",name=\"linear_output\")(top_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84d35b31-e9e5-4b8d-bf55-d714b9b86ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=efficientnet_model.input, outputs=top_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf6c2cc3-2cd2-4743-b7bc-9a33a05516f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f06e8102190>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the weights\n",
    "cp_dir = \"training\"\n",
    "weights = tf.train.latest_checkpoint(cp_dir)\n",
    "model.load_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a723cbd-c470-4228-b02b-4ddbd5ffdade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7c0e67a-fced-406c-af67-96d12e7be1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file=\"79466_2019_V9.040EU_V9.039EU.mpg\"\n",
    "video_clip = VideoFileClip(video_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d2e18fda-3aae-402a-b09c-b13393d982c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_frame_per_sec=1\n",
    "framing_rate = min(video_clip.fps, nb_frame_per_sec)\n",
    "step = 1 / video_clip.fps if framing_rate == 0 else 1 / framing_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "46238544-8d86-4788-a682-37fd36fe5899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new directory to store framed images\n",
    "filename, _ = os.path.splitext(video_file)\n",
    "filename += \"-framed\"\n",
    "if not os.path.isdir(filename):\n",
    "\tos.mkdir(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "14aaec4f-5755-466c-8813-6b2598a56a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_Fissure = []\n",
    "ts_Racines = []\n",
    "for current_duration in np.arange(0, video_clip.duration, step):\n",
    "    frame_filename = os.path.join(filename, f\"frame{current_duration}.jpg\")\n",
    "    # save the frame with the current duration\n",
    "    video_clip.save_frame(frame_filename, current_duration)\n",
    "    img = tf.keras.utils.load_img(frame_filename, target_size=(224, 224))\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "    predictions = model.predict(img_array)\n",
    "    \n",
    "    pred = np.round(predictions[0])\n",
    "\n",
    "    # need to know what labels [1, 0, 0], [0, 1, 0] and [0, 0, 1] correspond to\n",
    "    # Assuming [1, 0, 0] is Fissure and [0, 0, 1] correspond to Racines\n",
    "    if pred[0] == 1 and pred[1] == 0 and pred[2] == 0 :\n",
    "        mins = np.floor(current_duration/60)\n",
    "        secs = (current_duration/60 - mins)*60\n",
    "        ts_Fissure.append([mins, secs])\n",
    "        \n",
    "    elif pred[0] == 0 and pred[1] == 0 and pred[2] == 1 :\n",
    "        mins = np.floor(current_duration/60)\n",
    "        secs = (current_duration/60 - mins)*60\n",
    "        ts_Racines.append([mins, secs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2718dd7a-e471-43fe-b067-1da2dad2a409",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_Fissure=np.int_(np.floor(ts_Fissure))\n",
    "ts_Racines=np.int_(np.floor(ts_Racines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9344f947-8b1f-4e63-9d30-649cd758ed02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fissure found in time stamps:\n",
      "Racine found in time stamps:\n",
      "0'0\"\n",
      "0'1\"\n",
      "0'2\"\n",
      "0'8\"\n",
      "0'9\"\n",
      "0'10\"\n",
      "0'11\"\n",
      "0'12\"\n",
      "0'13\"\n",
      "0'14\"\n",
      "0'15\"\n",
      "0'16\"\n",
      "0'19\"\n",
      "0'36\"\n",
      "0'37\"\n",
      "0'38\"\n",
      "0'39\"\n",
      "0'40\"\n",
      "0'41\"\n",
      "0'42\"\n",
      "0'43\"\n",
      "0'44\"\n",
      "0'45\"\n",
      "0'46\"\n",
      "0'47\"\n",
      "0'48\"\n",
      "0'49\"\n",
      "0'50\"\n",
      "0'51\"\n",
      "0'52\"\n",
      "1'6\"\n",
      "1'7\"\n",
      "1'7\"\n",
      "1'8\"\n",
      "1'10\"\n",
      "1'11\"\n",
      "1'11\"\n",
      "1'12\"\n",
      "1'14\"\n",
      "1'15\"\n",
      "1'15\"\n",
      "1'17\"\n",
      "1'18\"\n",
      "1'19\"\n",
      "1'19\"\n",
      "1'21\"\n",
      "1'22\"\n",
      "1'23\"\n",
      "1'23\"\n",
      "1'25\"\n",
      "1'26\"\n",
      "1'26\"\n",
      "1'27\"\n",
      "1'29\"\n",
      "1'38\"\n",
      "1'38\"\n",
      "1'40\"\n",
      "1'55\"\n",
      "1'56\"\n",
      "1'57\"\n",
      "1'57\"\n",
      "1'59\"\n",
      "2'0\"\n",
      "2'0\"\n",
      "2'1\"\n",
      "2'9\"\n",
      "2'22\"\n",
      "2'23\"\n",
      "2'23\"\n",
      "2'24\"\n",
      "2'25\"\n",
      "2'27\"\n",
      "2'28\"\n",
      "2'29\"\n",
      "2'30\"\n",
      "2'30\"\n",
      "2'35\"\n",
      "2'36\"\n",
      "2'37\"\n",
      "2'38\"\n",
      "2'38\"\n",
      "2'39\"\n",
      "2'40\"\n",
      "2'42\"\n",
      "2'43\"\n",
      "2'44\"\n",
      "2'45\"\n",
      "2'46\"\n",
      "2'46\"\n",
      "2'47\"\n",
      "2'49\"\n",
      "2'50\"\n",
      "2'51\"\n",
      "2'52\"\n",
      "2'53\"\n",
      "2'53\"\n",
      "2'54\"\n",
      "2'55\"\n",
      "3'5\"\n",
      "3'6\"\n",
      "3'20\"\n",
      "3'21\"\n",
      "3'24\"\n",
      "3'25\"\n",
      "3'27\"\n",
      "3'28\"\n",
      "3'32\"\n",
      "3'34\"\n",
      "3'35\"\n",
      "3'38\"\n",
      "3'38\"\n",
      "3'39\"\n",
      "3'40\"\n",
      "3'59\"\n",
      "4'11\"\n",
      "4'12\"\n",
      "4'13\"\n",
      "4'14\"\n",
      "4'15\"\n",
      "4'15\"\n",
      "4'16\"\n",
      "4'17\"\n",
      "4'18\"\n",
      "4'19\"\n",
      "4'20\"\n",
      "4'21\"\n",
      "4'23\"\n",
      "4'25\"\n",
      "4'26\"\n",
      "5'11\"\n",
      "5'30\"\n",
      "5'31\"\n",
      "5'33\"\n",
      "5'41\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Fissure found in time stamps:')\n",
    "\"\"\"for timeStamps in ts_Fissure:\n",
    "    print('\\n')\n",
    "    print(timeStamps[0]+':'+timeStamps[1])\"\"\"\n",
    "\n",
    "print('Racine found in time stamps:')\n",
    "for timeStamps in ts_Racines:\n",
    "    #print('\\n')\n",
    "    print(str(timeStamps[0])+'\\''+str(timeStamps[1]) + '\\\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
