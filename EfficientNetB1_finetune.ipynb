{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nE_6M8YGgMIh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Activation, Dropout, Input, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, AveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the batches for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ch-P-C91gTNj"
   },
   "outputs": [],
   "source": [
    "#find data sets. note these are not made public due to confidential nature.\n",
    "train_path = \"train_data/train\"\n",
    "valid_path = \"train_data/valid\"\n",
    "test_path = \"train_data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ox2bcaX8gVPF"
   },
   "outputs": [],
   "source": [
    "#generation of batches; batch_size can be adapted based on availability of GPU.\n",
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.efficientnet.preprocess_input) \\\n",
    "    .flow_from_directory(directory=train_path, target_size=(240,240), classes=['Fissure', 'Racines', 'Normal'], batch_size=64)\n",
    "\n",
    "valid_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.efficientnet.preprocess_input) \\\n",
    "    .flow_from_directory(directory=valid_path, target_size=(240,240), classes=['Fissure', 'Racines', 'Normal'], batch_size=64)\n",
    "\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.efficientnet.preprocess_input) \\\n",
    "    .flow_from_directory(directory=test_path, target_size=(240,240), classes=['Fissure', 'Racines', 'Normal'], batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation (need internet connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Siv63cSRiifH"
   },
   "outputs": [],
   "source": [
    "#download the model, top is not included in order to do transfer learning.\n",
    "efficientnet_model = tf.keras.applications.efficientnet.EfficientNetB1(include_top=False,input_tensor=Input(shape=(224, 224, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FrACp41iq5S"
   },
   "outputs": [],
   "source": [
    "efficientnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWK-WaYCivoQ"
   },
   "outputs": [],
   "source": [
    "#additional dense layers and dropout to successfully carry out transfer learning. Original output is flattened such that the original model functions as feature extractor.\n",
    "top_layers = efficientnet_model.output\n",
    "top_layers = Flatten(name=\"flatten_top\")(top_layers)\n",
    "top_layers = Dense(1024, activation=\"ReLU\",name=\"first_dense_top\")(top_layers)\n",
    "top_layers = Dropout(0.5, name=\"dropout_top\")(top_layers)\n",
    "top_layers = Dense(units=3, activation=\"softmax\",name=\"linear_output\")(top_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWrZ4ryEjhyu"
   },
   "outputs": [],
   "source": [
    "#the efficientnet model and the output layers are joined together\n",
    "model = Model(inputs=efficientnet_model.input, outputs=top_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhM-hUSXjt6X"
   },
   "outputs": [],
   "source": [
    "#original model is set to be untrainable.\n",
    "for layer in efficientnet_model.layers:\n",
    "\tlayer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTmLLS7nj2WJ"
   },
   "outputs": [],
   "source": [
    "#it was previously determined which learning rate was optimal, this is implemented now.\n",
    "learning_rate = np.logspace(-4,-2,4)\n",
    "lr = learning_rate[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Z4dBkK4SS5h"
   },
   "outputs": [],
   "source": [
    "#implementation of early stopping based on the monitoring of \n",
    "stop = EarlyStopping(monitor='val_accuracy', mode='max', patience=2, min_delta=1, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3rj_clBkC_H"
   },
   "outputs": [],
   "source": [
    "#compile model and carry out transfer learning\n",
    "model.compile(optimizer=Adam(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_batches,\n",
    "          steps_per_epoch=len(train_batches),\n",
    "          validation_data=valid_batches,\n",
    "          validation_steps=len(valid_batches),\n",
    "          epochs=10,\n",
    "          callbacks=[stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y0xr8E3CTjnf"
   },
   "outputs": [],
   "source": [
    "#defining the directory to save the weights, use checkpoint to make sure optimal weights are saved.\n",
    "cp_path = \"training/weights.ckpt\"\n",
    "cp_directory = os.path.dirname(cp_path)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=cp_path,\n",
    "                              save_weights_only=True,\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "neeFA0BtJ0C8"
   },
   "outputs": [],
   "source": [
    "#unfreeze last 10 layers (except for the batch normalisations)\n",
    "for layer in model.layers[-10:]:\n",
    "  if layer != BatchNormalization:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w8qocLTgXmZG"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ki23-H0eJ5ew"
   },
   "outputs": [],
   "source": [
    "#further fine-tune the model to obtain the final weights\n",
    "model.fit(train_batches,\n",
    "          steps_per_epoch=len(train_batches),\n",
    "          validation_data=valid_batches,\n",
    "          validation_steps=len(valid_batches),\n",
    "          epochs=10,\n",
    "          callbacks = [stop,checkpoint]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EfficientNetB1_finetune.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
